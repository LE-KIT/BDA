{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Schlendrikovic/Documents/Python/VirtualEnvironments/DeepLearning/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense,Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data_preprocessed.csv\",sep=\";\",index_col=[0])\n",
    "\n",
    "df_prev_day_info = df.loc[:,'prev_day_consumption_ger':'12']\n",
    "df_prev_day_info['Tag'] = df['Tag']\n",
    "\n",
    "df['Tag'] = pd.to_datetime(df['Tag'])\n",
    "df_prev_day_info['Tag'] = pd.to_datetime(df_prev_day_info['Tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.DataFrame(index=np.arange(0,df.Tag.nunique()))\n",
    "df_master['Tag'] = None\n",
    "\n",
    "df_master['NX_per_country'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = df.loc[:,'NX':'PL'].mean()\n",
    "std_columns = df.loc[:,'NX':'PL'].std()\n",
    "\n",
    "max_columns =df.loc[:,'NX':'PL'].max()\n",
    "min_columns =df.loc[:,'NX':'PL'].min()\n",
    "\n",
    "#df.loc[:,'NX':'PL'] = (df.loc[:,'NX':'PL']-mean_columns)/std_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag weniger als 24 EInträge:2015-10-25 00:00:00 25\n",
      "Tag weniger als 24 EInträge:2016-03-27 00:00:00 23\n",
      "Tag weniger als 24 EInträge:2016-10-30 00:00:00 25\n",
      "Tag weniger als 24 EInträge:2017-03-26 00:00:00 23\n",
      "Tag weniger als 24 EInträge:2017-10-29 00:00:00 25\n",
      "Tag weniger als 24 EInträge:2018-03-25 00:00:00 23\n",
      "Tag weniger als 24 EInträge:2018-10-28 00:00:00 25\n",
      "Tag weniger als 24 EInträge:2019-03-31 00:00:00 23\n"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "for tag,group in df.groupby('Tag'):\n",
    "    if len(group)==24:\n",
    "        df_master.loc[ind,'Tag'] =  tag\n",
    "        df_master.loc[ind,'NX_per_country'] =  group.loc[:,'NX':'PL'].values\n",
    "    else:\n",
    "        df_master.loc[ind,'NX_per_country'] = df_master.loc[ind-1,'NX_per_country'] \n",
    "        df_master.loc[ind,'Tag'] = df_master.loc[ind-1,'Tag'] \n",
    "        print(\"Tag weniger als 24 EInträge:{} {}\".format(tag,len((group))))\n",
    "    ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master['NX_per_country_prev_day'] = df_master['NX_per_country'].shift(1)\n",
    "df_master.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master['Tag'] = pd.to_datetime(df_master['Tag'])\n",
    "\n",
    "df_master = df_master.merge(df_prev_day_info,on='Tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>NX_per_country</th>\n",
       "      <th>NX_per_country_prev_day</th>\n",
       "      <th>prev_day_consumption_ger</th>\n",
       "      <th>prev_day_price_deutschland/luxemburg</th>\n",
       "      <th>prev_day_price_dänemark 1</th>\n",
       "      <th>prev_day_price_dänemark 2</th>\n",
       "      <th>prev_day_price_frankreich</th>\n",
       "      <th>prev_day_price_italien (nord)</th>\n",
       "      <th>prev_day_price_niederlande</th>\n",
       "      <th>...</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-06-03</td>\n",
       "      <td>[[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....</td>\n",
       "      <td>[[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...</td>\n",
       "      <td>1365352.75</td>\n",
       "      <td>44.390017</td>\n",
       "      <td>16.130417</td>\n",
       "      <td>16.7625</td>\n",
       "      <td>24.302083</td>\n",
       "      <td>43.613333</td>\n",
       "      <td>33.039583</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-06-03</td>\n",
       "      <td>[[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....</td>\n",
       "      <td>[[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...</td>\n",
       "      <td>1365352.75</td>\n",
       "      <td>44.390017</td>\n",
       "      <td>16.130417</td>\n",
       "      <td>16.7625</td>\n",
       "      <td>24.302083</td>\n",
       "      <td>43.613333</td>\n",
       "      <td>33.039583</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-03</td>\n",
       "      <td>[[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....</td>\n",
       "      <td>[[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...</td>\n",
       "      <td>1365352.75</td>\n",
       "      <td>44.390017</td>\n",
       "      <td>16.130417</td>\n",
       "      <td>16.7625</td>\n",
       "      <td>24.302083</td>\n",
       "      <td>43.613333</td>\n",
       "      <td>33.039583</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-03</td>\n",
       "      <td>[[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....</td>\n",
       "      <td>[[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...</td>\n",
       "      <td>1365352.75</td>\n",
       "      <td>44.390017</td>\n",
       "      <td>16.130417</td>\n",
       "      <td>16.7625</td>\n",
       "      <td>24.302083</td>\n",
       "      <td>43.613333</td>\n",
       "      <td>33.039583</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-03</td>\n",
       "      <td>[[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....</td>\n",
       "      <td>[[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...</td>\n",
       "      <td>1365352.75</td>\n",
       "      <td>44.390017</td>\n",
       "      <td>16.130417</td>\n",
       "      <td>16.7625</td>\n",
       "      <td>24.302083</td>\n",
       "      <td>43.613333</td>\n",
       "      <td>33.039583</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tag                                     NX_per_country  \\\n",
       "0 2015-06-03  [[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....   \n",
       "1 2015-06-03  [[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....   \n",
       "2 2015-06-03  [[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....   \n",
       "3 2015-06-03  [[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....   \n",
       "4 2015-06-03  [[5107.0, 3482.0, -653.0, 34.0, 446.0, 0.0, 1....   \n",
       "\n",
       "                             NX_per_country_prev_day  \\\n",
       "0  [[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...   \n",
       "1  [[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...   \n",
       "2  [[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...   \n",
       "3  [[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...   \n",
       "4  [[680.0, 3068.0, -894.0, -1090.0, -341.0, 0.0,...   \n",
       "\n",
       "   prev_day_consumption_ger  prev_day_price_deutschland/luxemburg  \\\n",
       "0                1365352.75                             44.390017   \n",
       "1                1365352.75                             44.390017   \n",
       "2                1365352.75                             44.390017   \n",
       "3                1365352.75                             44.390017   \n",
       "4                1365352.75                             44.390017   \n",
       "\n",
       "   prev_day_price_dänemark 1  prev_day_price_dänemark 2  \\\n",
       "0                  16.130417                    16.7625   \n",
       "1                  16.130417                    16.7625   \n",
       "2                  16.130417                    16.7625   \n",
       "3                  16.130417                    16.7625   \n",
       "4                  16.130417                    16.7625   \n",
       "\n",
       "   prev_day_price_frankreich  prev_day_price_italien (nord)  \\\n",
       "0                  24.302083                      43.613333   \n",
       "1                  24.302083                      43.613333   \n",
       "2                  24.302083                      43.613333   \n",
       "3                  24.302083                      43.613333   \n",
       "4                  24.302083                      43.613333   \n",
       "\n",
       "   prev_day_price_niederlande  ...  3  4  5  6  7  8  9  10  11  12  \n",
       "0                   33.039583  ...  0  0  0  1  0  0  0   0   0   0  \n",
       "1                   33.039583  ...  0  0  0  1  0  0  0   0   0   0  \n",
       "2                   33.039583  ...  0  0  0  1  0  0  0   0   0   0  \n",
       "3                   33.039583  ...  0  0  0  1  0  0  0   0   0   0  \n",
       "4                   33.039583  ...  0  0  0  1  0  0  0   0   0   0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim,regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth=1,filters=(16, 32, 64),regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    x = inputs\n",
    "    \n",
    "    x = Conv2D(16,kernel_size=(4,1),strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv2D(32,kernel_size=(1,width),strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    \n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(height*width*2)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data \n",
    "df_cnn = df_master[['Tag','NX_per_country_prev_day']]\n",
    "df_mlp = df_master.drop('NX_per_country_prev_day',axis=1)\n",
    "\n",
    "split = train_test_split(df_mlp,df_cnn, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "\n",
    "\n",
    "testY = testAttrX[['NX_per_country']]\n",
    "trainY = trainAttrX[['NX_per_country']]\n",
    "\n",
    "trainAttrX.drop(['Tag','NX_per_country'],axis=1,inplace=True)\n",
    "testAttrX.drop(['Tag','NX_per_country'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(trainAttrX.shape[1])\n",
    "cnn = create_cnn(10, 24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Nets\n",
    "\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "\n",
    "x = Dense(24*10,kernel_initializer='normal')(combinedInput)\n",
    "x = Reshape((24, 10))(x)\n",
    " \n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "model.compile(loss=\"mean_absolute_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "trainY = np.array(trainY['NX_per_country'].tolist())\n",
    "testY = np.array(testY['NX_per_country'].tolist())\n",
    "\n",
    "trainImagesX = np.expand_dims(np.array(trainImagesX['NX_per_country_prev_day'].tolist()), axis=3)\n",
    "testImagesX = np.expand_dims(np.array(testImagesX['NX_per_country_prev_day'].tolist()), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Train on 26280 samples, validate on 8760 samples\n",
      "Epoch 1/100\n",
      "26280/26280 [==============================] - 182s 7ms/step - loss: 606.8155 - val_loss: 531.3986\n",
      "Epoch 2/100\n",
      "26280/26280 [==============================] - 183s 7ms/step - loss: 456.3679 - val_loss: 386.7923\n",
      "Epoch 3/100\n",
      "26280/26280 [==============================] - 208s 8ms/step - loss: 369.8594 - val_loss: 323.7356\n",
      "Epoch 4/100\n",
      "26280/26280 [==============================] - 225s 9ms/step - loss: 315.0421 - val_loss: 303.6883\n",
      "Epoch 5/100\n",
      "26280/26280 [==============================] - 234s 9ms/step - loss: 281.7729 - val_loss: 272.2760\n",
      "Epoch 6/100\n",
      "26280/26280 [==============================] - 213s 8ms/step - loss: 271.1008 - val_loss: 264.9025\n",
      "Epoch 7/100\n",
      "26280/26280 [==============================] - 197s 8ms/step - loss: 254.9510 - val_loss: 225.2892\n",
      "Epoch 8/100\n",
      "26280/26280 [==============================] - 210s 8ms/step - loss: 237.0795 - val_loss: 242.2735\n",
      "Epoch 9/100\n",
      "26280/26280 [==============================] - 185s 7ms/step - loss: 231.1804 - val_loss: 224.3087\n",
      "Epoch 10/100\n",
      "26280/26280 [==============================] - 182s 7ms/step - loss: 228.1161 - val_loss: 195.9865\n",
      "Epoch 11/100\n",
      "26280/26280 [==============================] - 185s 7ms/step - loss: 220.6842 - val_loss: 230.3690\n",
      "Epoch 12/100\n",
      "26280/26280 [==============================] - 183s 7ms/step - loss: 209.8970 - val_loss: 198.9183\n",
      "Epoch 13/100\n",
      "26280/26280 [==============================] - 185s 7ms/step - loss: 212.8615 - val_loss: 231.6312\n",
      "Epoch 14/100\n",
      "26280/26280 [==============================] - 185s 7ms/step - loss: 201.6392 - val_loss: 201.2122\n",
      "Epoch 15/100\n",
      "26280/26280 [==============================] - 187s 7ms/step - loss: 197.5098 - val_loss: 198.6272\n",
      "Epoch 16/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 198.2031 - val_loss: 208.3137\n",
      "Epoch 17/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 192.3406 - val_loss: 246.4767\n",
      "Epoch 18/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 194.5430 - val_loss: 234.5348\n",
      "Epoch 19/100\n",
      "26280/26280 [==============================] - 189s 7ms/step - loss: 186.0227 - val_loss: 188.3028\n",
      "Epoch 20/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 186.5277 - val_loss: 202.1881\n",
      "Epoch 21/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 180.7680 - val_loss: 223.4688\n",
      "Epoch 22/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 180.3195 - val_loss: 213.6003\n",
      "Epoch 23/100\n",
      "26280/26280 [==============================] - 191s 7ms/step - loss: 178.6102 - val_loss: 218.5697\n",
      "Epoch 24/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 173.6697 - val_loss: 209.0238\n",
      "Epoch 25/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 175.9647 - val_loss: 214.0227\n",
      "Epoch 26/100\n",
      "26280/26280 [==============================] - 188s 7ms/step - loss: 173.0059 - val_loss: 202.9010\n",
      "Epoch 27/100\n",
      "26280/26280 [==============================] - 189s 7ms/step - loss: 169.6212 - val_loss: 214.3954\n",
      "Epoch 28/100\n",
      "26280/26280 [==============================] - 191s 7ms/step - loss: 170.4032 - val_loss: 215.1530\n",
      "Epoch 29/100\n",
      "26280/26280 [==============================] - 190s 7ms/step - loss: 168.1202 - val_loss: 211.8997\n",
      "Epoch 30/100\n",
      "26280/26280 [==============================] - 190s 7ms/step - loss: 166.8365 - val_loss: 222.3636\n",
      "Epoch 31/100\n",
      "26280/26280 [==============================] - 190s 7ms/step - loss: 164.2062 - val_loss: 219.3324\n",
      "Epoch 32/100\n",
      "26280/26280 [==============================] - 190s 7ms/step - loss: 167.1756 - val_loss: 203.3873\n",
      "Epoch 33/100\n",
      "26280/26280 [==============================] - 190s 7ms/step - loss: 166.8733 - val_loss: 260.2805\n",
      "Epoch 34/100\n",
      "26280/26280 [==============================] - 191s 7ms/step - loss: 159.3339 - val_loss: 232.2720\n",
      "Epoch 35/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 157.9122 - val_loss: 201.4344\n",
      "Epoch 36/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 156.5995 - val_loss: 211.7882\n",
      "Epoch 37/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 157.0544 - val_loss: 198.5136\n",
      "Epoch 38/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 158.0881 - val_loss: 223.3482\n",
      "Epoch 39/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 154.9692 - val_loss: 220.0147\n",
      "Epoch 40/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 155.9883 - val_loss: 226.3930\n",
      "Epoch 41/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 154.3415 - val_loss: 225.8851\n",
      "Epoch 42/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 152.4045 - val_loss: 230.9048\n",
      "Epoch 43/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 152.0689 - val_loss: 228.3981\n",
      "Epoch 44/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 151.1827 - val_loss: 231.9567\n",
      "Epoch 45/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 153.6001 - val_loss: 202.9856\n",
      "Epoch 46/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 149.2016 - val_loss: 202.8251\n",
      "Epoch 47/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 146.7512 - val_loss: 212.8843\n",
      "Epoch 48/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 147.4918 - val_loss: 223.9950\n",
      "Epoch 49/100\n",
      "26280/26280 [==============================] - 196s 7ms/step - loss: 150.0997 - val_loss: 216.0466\n",
      "Epoch 50/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 148.5911 - val_loss: 250.1755\n",
      "Epoch 51/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 146.7579 - val_loss: 234.0649\n",
      "Epoch 52/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 145.7221 - val_loss: 211.5747\n",
      "Epoch 53/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 143.4110 - val_loss: 224.2082\n",
      "Epoch 54/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 141.7844 - val_loss: 223.4541\n",
      "Epoch 55/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 141.6918 - val_loss: 218.1497\n",
      "Epoch 56/100\n",
      "26280/26280 [==============================] - 196s 7ms/step - loss: 145.8277 - val_loss: 222.1125\n",
      "Epoch 57/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 142.0069 - val_loss: 219.2753\n",
      "Epoch 58/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 141.6805 - val_loss: 217.1949\n",
      "Epoch 59/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 140.9274 - val_loss: 229.3302\n",
      "Epoch 60/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 141.7407 - val_loss: 213.1666\n",
      "Epoch 61/100\n",
      "26280/26280 [==============================] - 196s 7ms/step - loss: 140.6905 - val_loss: 221.9249\n",
      "Epoch 62/100\n",
      "26280/26280 [==============================] - 196s 7ms/step - loss: 137.9371 - val_loss: 215.5243\n",
      "Epoch 63/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 140.5529 - val_loss: 225.9689\n",
      "Epoch 64/100\n",
      "26280/26280 [==============================] - 198s 8ms/step - loss: 136.4928 - val_loss: 215.5857\n",
      "Epoch 65/100\n",
      "26280/26280 [==============================] - 196s 7ms/step - loss: 137.3759 - val_loss: 225.1130\n",
      "Epoch 66/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 136.2393 - val_loss: 213.0728\n",
      "Epoch 67/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 136.3108 - val_loss: 217.9268\n",
      "Epoch 68/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 135.6630 - val_loss: 222.6257\n",
      "Epoch 69/100\n",
      "26280/26280 [==============================] - 197s 7ms/step - loss: 134.3224 - val_loss: 212.4234\n",
      "Epoch 70/100\n",
      "26280/26280 [==============================] - 196s 7ms/step - loss: 137.8174 - val_loss: 222.0276\n",
      "Epoch 71/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 134.7954 - val_loss: 222.8663\n",
      "Epoch 72/100\n",
      "26280/26280 [==============================] - 198s 8ms/step - loss: 133.9089 - val_loss: 217.0267\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26280/26280 [==============================] - 191s 7ms/step - loss: 141.4413 - val_loss: 208.4592\n",
      "Epoch 74/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 136.5471 - val_loss: 210.5477\n",
      "Epoch 75/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 133.2146 - val_loss: 221.3192\n",
      "Epoch 76/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 132.2049 - val_loss: 209.6082\n",
      "Epoch 77/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 132.0366 - val_loss: 222.9106\n",
      "Epoch 78/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 130.8483 - val_loss: 214.7846\n",
      "Epoch 79/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 130.5601 - val_loss: 241.4036\n",
      "Epoch 80/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 138.5045 - val_loss: 215.6285\n",
      "Epoch 81/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 132.3224 - val_loss: 217.6356\n",
      "Epoch 82/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 131.0803 - val_loss: 198.5541\n",
      "Epoch 83/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 130.1970 - val_loss: 220.3871\n",
      "Epoch 84/100\n",
      "26280/26280 [==============================] - 192s 7ms/step - loss: 130.0720 - val_loss: 205.4875\n",
      "Epoch 85/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 129.4267 - val_loss: 213.3091\n",
      "Epoch 86/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 130.1381 - val_loss: 198.8531\n",
      "Epoch 87/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 127.9977 - val_loss: 196.1123\n",
      "Epoch 88/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 128.3917 - val_loss: 202.5024\n",
      "Epoch 89/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 126.9301 - val_loss: 197.7206\n",
      "Epoch 90/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 128.6904 - val_loss: 218.2059\n",
      "Epoch 91/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 128.7424 - val_loss: 220.2946\n",
      "Epoch 92/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 126.4750 - val_loss: 203.6994\n",
      "Epoch 93/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 125.6005 - val_loss: 206.7480\n",
      "Epoch 94/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 127.4266 - val_loss: 209.4814\n",
      "Epoch 95/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 124.9331 - val_loss: 192.7904\n",
      "Epoch 96/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 124.5453 - val_loss: 193.9327\n",
      "Epoch 97/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 124.1130 - val_loss: 197.8648\n",
      "Epoch 98/100\n",
      "26280/26280 [==============================] - 195s 7ms/step - loss: 124.4248 - val_loss: 204.3450\n",
      "Epoch 99/100\n",
      "26280/26280 [==============================] - 194s 7ms/step - loss: 123.3117 - val_loss: 204.6161\n",
      "Epoch 100/100\n",
      "26280/26280 [==============================] - 193s 7ms/step - loss: 124.2525 - val_loss: 209.5094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125ef9fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "    [trainAttrX, trainImagesX], trainY,\n",
    "    validation_data=([testAttrX, testImagesX], testY),\n",
    "    epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/parallel_mlp_cnn_without_normalization.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([testAttrX, testImagesX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame(columns=df.columns[2:12])\n",
    "df_true_test_values = pd.DataFrame(columns=df.columns[2:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = split[3]['Tag'].reset_index(drop=True)\n",
    "ind = 0 \n",
    "\n",
    "\n",
    "for pred in preds:\n",
    "    data = pd.DataFrame(data=pred,columns=df.columns[2:12])\n",
    "    data['Tag'] = test_index[ind] + np.arange(24) * datetime.timedelta(hours=1)\n",
    "\n",
    "    if ind==0:\n",
    "        df_predictions = data\n",
    "    \n",
    "    df_predictions = df_predictions.append(data)\n",
    "    \n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_predictions.iloc[:,0:9] = (df_predictions.iloc[:,0:9]*std_columns) + mean_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = split[3]['Tag'].reset_index(drop=True)\n",
    "ind = 0 \n",
    "\n",
    "\n",
    "for test in testY:\n",
    "    \n",
    "    data = pd.DataFrame(data=test,columns=df.columns[2:12])\n",
    "    \n",
    "    data['Tag'] = test_index[ind] + np.arange(24) * datetime.timedelta(hours=1)\n",
    "\n",
    "    if ind==0:\n",
    "        df_true_test_values = data\n",
    "    \n",
    "    df_true_test_values = df_true_test_values.append(data)\n",
    "    \n",
    "    ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_true_test_values.iloc[:,0:9] = (df_true_test_values.iloc[:,0:9]*std_columns) + mean_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50666433.339347675"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df_true_test_values.iloc[:,0:9] - df_predictions.iloc[:,0:9])**2).mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10071.649"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions.loc[:,'NL':'PL'].iloc[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10148.667"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions.loc[:,'NX'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
