{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense,Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate\n",
    "import numpy as np\n",
    "import argparse\n",
    "import locale\n",
    "import os\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/stromfluesse.csv\",sep=\";\",index_col=[0])\n",
    "df['Tag'] = pd.to_datetime(df['Tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.DataFrame(index=np.arange(0,df.Tag.nunique()))\n",
    "df_master['Tag'] = None\n",
    "df_master['NX_per_country'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = df.iloc[:,2::].mean()\n",
    "std_columns = df.iloc[:,2::].std()\n",
    "\n",
    "max_columns =df.iloc[:,2::].max()\n",
    "min_columns =df.iloc[:,2::].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,2::] = (df.iloc[:,2::]-mean_columns)/std_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag weniger als 24 EInträge:2015-10-25 00:00:00 27\n",
      "Tag weniger als 24 EInträge:2016-03-27 00:00:00 23\n",
      "Tag weniger als 24 EInträge:2016-10-30 00:00:00 27\n",
      "Tag weniger als 24 EInträge:2017-03-26 00:00:00 23\n",
      "Tag weniger als 24 EInträge:2017-10-29 00:00:00 27\n",
      "Tag weniger als 24 EInträge:2018-03-25 00:00:00 23\n",
      "Tag weniger als 24 EInträge:2018-10-28 00:00:00 27\n",
      "Tag weniger als 24 EInträge:2019-03-31 00:00:00 23\n"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "for tag,group in df.groupby('Tag'):\n",
    "    if len(group)==24:\n",
    "        df_master.loc[ind,'Tag'] =  tag\n",
    "        df_master.loc[ind,'NX_per_country'] =  group.loc[:,'NX':'PL_IM'].values\n",
    "    else:\n",
    "        df_master.loc[ind,'NX_per_country'] = df_master.loc[ind-1,'NX_per_country'] \n",
    "        df_master.loc[ind,'Tag'] = df_master.loc[ind-1,'Tag'] \n",
    "        print(\"Tag weniger als 24 EInträge:{} {}\".format(tag,len((group))))\n",
    "    ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master['NX_per_country_prev_day'] = df_master['NX_per_country'].shift(1)\n",
    "df_master.dropna(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>NX_per_country</th>\n",
       "      <th>NX_per_country_prev_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>Time_in_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-06-03 00:00:00</td>\n",
       "      <td>[[-0.04643867358195919, 1.5857080002072266, 0....</td>\n",
       "      <td>[[-0.3720237825593603, 1.1545892851259032, 0.1...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-04 00:00:00</td>\n",
       "      <td>[[-0.44902572723958145, 1.126472847185817, 0.1...</td>\n",
       "      <td>[[-0.04643867358195919, 1.5857080002072266, 0....</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-05 00:00:00</td>\n",
       "      <td>[[-0.07659225268309641, 2.719737663790707, 0.1...</td>\n",
       "      <td>[[-0.44902572723958145, 1.126472847185817, 0.1...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-06 00:00:00</td>\n",
       "      <td>[[-0.10505428954197471, 2.5166745008900837, 0....</td>\n",
       "      <td>[[-0.07659225268309641, 2.719737663790707, 0.1...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06-07 00:00:00</td>\n",
       "      <td>[[-0.18602768122576027, 2.4469040808165365, 0....</td>\n",
       "      <td>[[-0.10505428954197471, 2.5166745008900837, 0....</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tag                                     NX_per_country  \\\n",
       "1  2015-06-03 00:00:00  [[-0.04643867358195919, 1.5857080002072266, 0....   \n",
       "2  2015-06-04 00:00:00  [[-0.44902572723958145, 1.126472847185817, 0.1...   \n",
       "3  2015-06-05 00:00:00  [[-0.07659225268309641, 2.719737663790707, 0.1...   \n",
       "4  2015-06-06 00:00:00  [[-0.10505428954197471, 2.5166745008900837, 0....   \n",
       "5  2015-06-07 00:00:00  [[-0.18602768122576027, 2.4469040808165365, 0....   \n",
       "\n",
       "                             NX_per_country_prev_day  day_of_week  \\\n",
       "1  [[-0.3720237825593603, 1.1545892851259032, 0.1...            2   \n",
       "2  [[-0.04643867358195919, 1.5857080002072266, 0....            3   \n",
       "3  [[-0.44902572723958145, 1.126472847185817, 0.1...            4   \n",
       "4  [[-0.07659225268309641, 2.719737663790707, 0.1...            5   \n",
       "5  [[-0.10505428954197471, 2.5166745008900837, 0....            6   \n",
       "\n",
       "   Time_in_seconds  \n",
       "1                0  \n",
       "2                1  \n",
       "3                2  \n",
       "4                3  \n",
       "5                4  "
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim,regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth=1,filters=(16, 32, 64),regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "\n",
    "    x = inputs\n",
    "    \n",
    "    x = Conv2D(16,kernel_size=(4,1),strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv2D(32,kernel_size=(1,width),strides=1, padding=\"same\")(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    \n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(height*width*2)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Schlendrikovic/Documents/Python/VirtualEnvironments/DeepLearning/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Data \n",
    "df_mlp = df_master[['Tag','day_of_week','Time_in_seconds','NX_per_country']]\n",
    "df_cnn = df_master[['Tag','NX_per_country_prev_day']]\n",
    "\n",
    "split = train_test_split(df_mlp,df_cnn, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split\n",
    "\n",
    "\n",
    "testY = testAttrX[['NX_per_country']]\n",
    "trainY = trainAttrX[['NX_per_country']]\n",
    "\n",
    "trainAttrX.drop(['Tag','NX_per_country'],axis=1,inplace=True)\n",
    "testAttrX.drop(['Tag','NX_per_country'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = create_mlp(trainAttrX.shape[1])\n",
    "cnn = create_cnn(19, 24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Nets\n",
    "\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "\n",
    "x = Dense(24*19,kernel_initializer='normal')(combinedInput)\n",
    "x = Reshape((24, 19))(x)\n",
    " \n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = Adam(lr=1e-3,decay=)\n",
    "model.compile(loss=\"mean_absolute_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape\n",
    "trainY = np.array(trainY['NX_per_country'].tolist())\n",
    "testY = np.array(testY['NX_per_country'].tolist())\n",
    "\n",
    "trainImagesX = np.expand_dims(np.array(trainImagesX['NX_per_country_prev_day'].tolist()), axis=3)\n",
    "testImagesX = np.expand_dims(np.array(testImagesX['NX_per_country_prev_day'].tolist()), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "Train on 1095 samples, validate on 366 samples\n",
      "Epoch 1/200\n",
      "1095/1095 [==============================] - 27s 25ms/step - loss: 1.8900 - val_loss: 0.6846\n",
      "Epoch 2/200\n",
      "1095/1095 [==============================] - 25s 23ms/step - loss: 0.5165 - val_loss: 0.4998\n",
      "Epoch 3/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.4436 - val_loss: 0.4568\n",
      "Epoch 4/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.4174 - val_loss: 0.4470\n",
      "Epoch 5/200\n",
      "1095/1095 [==============================] - 25s 23ms/step - loss: 0.3985 - val_loss: 0.4387\n",
      "Epoch 6/200\n",
      "1095/1095 [==============================] - 25s 23ms/step - loss: 0.3835 - val_loss: 0.4349\n",
      "Epoch 7/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.3718 - val_loss: 0.4192\n",
      "Epoch 8/200\n",
      "1095/1095 [==============================] - 25s 23ms/step - loss: 0.3602 - val_loss: 0.4153\n",
      "Epoch 9/200\n",
      "1095/1095 [==============================] - 25s 23ms/step - loss: 0.3495 - val_loss: 0.4107\n",
      "Epoch 10/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.3406 - val_loss: 0.4125\n",
      "Epoch 11/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.3312 - val_loss: 0.4120\n",
      "Epoch 12/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.3195 - val_loss: 0.4081\n",
      "Epoch 13/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.3120 - val_loss: 0.4067\n",
      "Epoch 14/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.3025 - val_loss: 0.4052\n",
      "Epoch 15/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2946 - val_loss: 0.4033\n",
      "Epoch 16/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2845 - val_loss: 0.4035\n",
      "Epoch 17/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2801 - val_loss: 0.4032\n",
      "Epoch 18/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2706 - val_loss: 0.4041\n",
      "Epoch 19/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2665 - val_loss: 0.4018\n",
      "Epoch 20/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2567 - val_loss: 0.4035\n",
      "Epoch 21/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2519 - val_loss: 0.4091\n",
      "Epoch 22/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2424 - val_loss: 0.4072\n",
      "Epoch 23/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2378 - val_loss: 0.4038\n",
      "Epoch 24/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2308 - val_loss: 0.4094\n",
      "Epoch 25/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2272 - val_loss: 0.4073\n",
      "Epoch 26/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2203 - val_loss: 0.4066\n",
      "Epoch 27/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.2153 - val_loss: 0.4074\n",
      "Epoch 28/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2126 - val_loss: 0.4057\n",
      "Epoch 29/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.2084 - val_loss: 0.4072\n",
      "Epoch 30/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.2044 - val_loss: 0.4031\n",
      "Epoch 31/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1990 - val_loss: 0.4055\n",
      "Epoch 32/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1939 - val_loss: 0.4047\n",
      "Epoch 33/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1942 - val_loss: 0.4091\n",
      "Epoch 34/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1893 - val_loss: 0.4073\n",
      "Epoch 35/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1846 - val_loss: 0.4070\n",
      "Epoch 36/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1829 - val_loss: 0.4073\n",
      "Epoch 37/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1784 - val_loss: 0.4051\n",
      "Epoch 38/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1762 - val_loss: 0.4057\n",
      "Epoch 39/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1748 - val_loss: 0.4068\n",
      "Epoch 40/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1726 - val_loss: 0.4047\n",
      "Epoch 41/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1706 - val_loss: 0.4055\n",
      "Epoch 42/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1698 - val_loss: 0.4066\n",
      "Epoch 43/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1695 - val_loss: 0.4069\n",
      "Epoch 44/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1635 - val_loss: 0.4055\n",
      "Epoch 45/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1613 - val_loss: 0.4030\n",
      "Epoch 46/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1580 - val_loss: 0.4064\n",
      "Epoch 47/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1578 - val_loss: 0.4051\n",
      "Epoch 48/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1555 - val_loss: 0.4036\n",
      "Epoch 49/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1547 - val_loss: 0.4066\n",
      "Epoch 50/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1519 - val_loss: 0.4040\n",
      "Epoch 51/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1509 - val_loss: 0.4048\n",
      "Epoch 52/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1501 - val_loss: 0.4072\n",
      "Epoch 53/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1477 - val_loss: 0.4047\n",
      "Epoch 54/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1459 - val_loss: 0.4071\n",
      "Epoch 55/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1466 - val_loss: 0.4054\n",
      "Epoch 56/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1432 - val_loss: 0.4065\n",
      "Epoch 57/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1425 - val_loss: 0.4052\n",
      "Epoch 58/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1412 - val_loss: 0.4051\n",
      "Epoch 59/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1390 - val_loss: 0.4054\n",
      "Epoch 60/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1404 - val_loss: 0.4052\n",
      "Epoch 61/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1415 - val_loss: 0.4061\n",
      "Epoch 62/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1383 - val_loss: 0.4081\n",
      "Epoch 63/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1354 - val_loss: 0.4042\n",
      "Epoch 64/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1352 - val_loss: 0.4055\n",
      "Epoch 65/200\n",
      "1095/1095 [==============================] - 27s 24ms/step - loss: 0.1330 - val_loss: 0.4060\n",
      "Epoch 66/200\n",
      "1095/1095 [==============================] - 27s 24ms/step - loss: 0.1354 - val_loss: 0.4074\n",
      "Epoch 67/200\n",
      "1095/1095 [==============================] - 27s 24ms/step - loss: 0.1318 - val_loss: 0.4051\n",
      "Epoch 68/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1319 - val_loss: 0.4060\n",
      "Epoch 69/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1314 - val_loss: 0.4042\n",
      "Epoch 70/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1288 - val_loss: 0.4037\n",
      "Epoch 71/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1281 - val_loss: 0.4044\n",
      "Epoch 72/200\n",
      "1095/1095 [==============================] - 27s 24ms/step - loss: 0.1284 - val_loss: 0.4057\n",
      "Epoch 73/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1289 - val_loss: 0.4051\n",
      "Epoch 74/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1274 - val_loss: 0.4054\n",
      "Epoch 75/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1274 - val_loss: 0.4056\n",
      "Epoch 76/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1271 - val_loss: 0.4048\n",
      "Epoch 77/200\n",
      "1095/1095 [==============================] - 27s 24ms/step - loss: 0.1266 - val_loss: 0.4064\n",
      "Epoch 78/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1252 - val_loss: 0.4050\n",
      "Epoch 79/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1228 - val_loss: 0.4063\n",
      "Epoch 80/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1222 - val_loss: 0.4051\n",
      "Epoch 81/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1216 - val_loss: 0.4049\n",
      "Epoch 82/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1218 - val_loss: 0.4051\n",
      "Epoch 83/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1199 - val_loss: 0.4042\n",
      "Epoch 84/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1201 - val_loss: 0.4045\n",
      "Epoch 85/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1191 - val_loss: 0.4028\n",
      "Epoch 86/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1188 - val_loss: 0.4047\n",
      "Epoch 87/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1185 - val_loss: 0.4049\n",
      "Epoch 88/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1186 - val_loss: 0.4027\n",
      "Epoch 89/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1178 - val_loss: 0.4047\n",
      "Epoch 90/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1169 - val_loss: 0.4025\n",
      "Epoch 91/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1170 - val_loss: 0.4049\n",
      "Epoch 92/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1166 - val_loss: 0.4027\n",
      "Epoch 93/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1164 - val_loss: 0.4022\n",
      "Epoch 94/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1158 - val_loss: 0.4042\n",
      "Epoch 95/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1146 - val_loss: 0.4018\n",
      "Epoch 96/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1143 - val_loss: 0.4035\n",
      "Epoch 97/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1138 - val_loss: 0.4047\n",
      "Epoch 98/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1138 - val_loss: 0.4057\n",
      "Epoch 99/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1139 - val_loss: 0.4054\n",
      "Epoch 100/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1129 - val_loss: 0.4035\n",
      "Epoch 101/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1131 - val_loss: 0.4032\n",
      "Epoch 102/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1138 - val_loss: 0.4034\n",
      "Epoch 103/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1119 - val_loss: 0.4037\n",
      "Epoch 104/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1111 - val_loss: 0.4036\n",
      "Epoch 105/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1106 - val_loss: 0.4058\n",
      "Epoch 106/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1114 - val_loss: 0.4037\n",
      "Epoch 107/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1117 - val_loss: 0.4047\n",
      "Epoch 108/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1104 - val_loss: 0.4040\n",
      "Epoch 109/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1093 - val_loss: 0.4045\n",
      "Epoch 110/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1076 - val_loss: 0.4041\n",
      "Epoch 111/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1075 - val_loss: 0.4043\n",
      "Epoch 112/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.1092 - val_loss: 0.4039\n",
      "Epoch 113/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1081 - val_loss: 0.4039\n",
      "Epoch 114/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1075 - val_loss: 0.4050\n",
      "Epoch 115/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1065 - val_loss: 0.4038\n",
      "Epoch 116/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1058 - val_loss: 0.4027\n",
      "Epoch 117/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1065 - val_loss: 0.4038\n",
      "Epoch 118/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1062 - val_loss: 0.4043\n",
      "Epoch 119/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1059 - val_loss: 0.4041\n",
      "Epoch 120/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1069 - val_loss: 0.4038\n",
      "Epoch 121/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1058 - val_loss: 0.4031\n",
      "Epoch 122/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1053 - val_loss: 0.4043\n",
      "Epoch 123/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1042 - val_loss: 0.4039\n",
      "Epoch 124/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1039 - val_loss: 0.4035\n",
      "Epoch 125/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1029 - val_loss: 0.4036\n",
      "Epoch 126/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1034 - val_loss: 0.4045\n",
      "Epoch 127/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1032 - val_loss: 0.4043\n",
      "Epoch 128/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1030 - val_loss: 0.4050\n",
      "Epoch 129/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1032 - val_loss: 0.4043\n",
      "Epoch 130/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1019 - val_loss: 0.4049\n",
      "Epoch 131/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1028 - val_loss: 0.4018\n",
      "Epoch 132/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1025 - val_loss: 0.4025\n",
      "Epoch 133/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1017 - val_loss: 0.4030\n",
      "Epoch 134/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1030 - val_loss: 0.4039\n",
      "Epoch 135/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1030 - val_loss: 0.4040\n",
      "Epoch 136/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1018 - val_loss: 0.4041\n",
      "Epoch 137/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1016 - val_loss: 0.4060\n",
      "Epoch 138/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1005 - val_loss: 0.4042\n",
      "Epoch 139/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1008 - val_loss: 0.4029\n",
      "Epoch 140/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1003 - val_loss: 0.4054\n",
      "Epoch 141/200\n",
      "1095/1095 [==============================] - 27s 25ms/step - loss: 0.1026 - val_loss: 0.4033\n",
      "Epoch 142/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1011 - val_loss: 0.4048\n",
      "Epoch 143/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1004 - val_loss: 0.4029\n",
      "Epoch 144/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0991 - val_loss: 0.4042\n",
      "Epoch 145/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.1000 - val_loss: 0.4053\n",
      "Epoch 146/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0996 - val_loss: 0.4039\n",
      "Epoch 147/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0987 - val_loss: 0.4029\n",
      "Epoch 148/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0991 - val_loss: 0.4037\n",
      "Epoch 149/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0990 - val_loss: 0.4048\n",
      "Epoch 150/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0983 - val_loss: 0.4038\n",
      "Epoch 151/200\n",
      "1095/1095 [==============================] - 27s 24ms/step - loss: 0.0973 - val_loss: 0.4027\n",
      "Epoch 152/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0988 - val_loss: 0.4043\n",
      "Epoch 153/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0973 - val_loss: 0.4034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.0976 - val_loss: 0.4039\n",
      "Epoch 155/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0974 - val_loss: 0.4041\n",
      "Epoch 156/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0973 - val_loss: 0.4029\n",
      "Epoch 157/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0967 - val_loss: 0.4021\n",
      "Epoch 158/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.0966 - val_loss: 0.4032\n",
      "Epoch 159/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0957 - val_loss: 0.4046\n",
      "Epoch 160/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0962 - val_loss: 0.4031\n",
      "Epoch 161/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.0958 - val_loss: 0.4038\n",
      "Epoch 162/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0963 - val_loss: 0.4034\n",
      "Epoch 163/200\n",
      "1095/1095 [==============================] - 26s 23ms/step - loss: 0.0957 - val_loss: 0.4038\n",
      "Epoch 164/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0965 - val_loss: 0.4037\n",
      "Epoch 165/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0956 - val_loss: 0.4033\n",
      "Epoch 166/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0954 - val_loss: 0.4038\n",
      "Epoch 167/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0955 - val_loss: 0.4036\n",
      "Epoch 168/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0948 - val_loss: 0.4058\n",
      "Epoch 169/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0948 - val_loss: 0.4034\n",
      "Epoch 170/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0946 - val_loss: 0.4031\n",
      "Epoch 171/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0957 - val_loss: 0.4044\n",
      "Epoch 172/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0948 - val_loss: 0.4049\n",
      "Epoch 173/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0940 - val_loss: 0.4020\n",
      "Epoch 174/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0932 - val_loss: 0.4047\n",
      "Epoch 175/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0947 - val_loss: 0.4046\n",
      "Epoch 176/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0934 - val_loss: 0.4028\n",
      "Epoch 177/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0935 - val_loss: 0.4038\n",
      "Epoch 178/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0937 - val_loss: 0.4045\n",
      "Epoch 179/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0926 - val_loss: 0.4031\n",
      "Epoch 180/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0934 - val_loss: 0.4031\n",
      "Epoch 181/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0932 - val_loss: 0.4048\n",
      "Epoch 182/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0934 - val_loss: 0.4029\n",
      "Epoch 183/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0927 - val_loss: 0.4050\n",
      "Epoch 184/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0922 - val_loss: 0.4022\n",
      "Epoch 185/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0929 - val_loss: 0.4054\n",
      "Epoch 186/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0924 - val_loss: 0.4032\n",
      "Epoch 187/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0923 - val_loss: 0.4038\n",
      "Epoch 188/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0917 - val_loss: 0.4049\n",
      "Epoch 189/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0921 - val_loss: 0.4030\n",
      "Epoch 190/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0912 - val_loss: 0.4026\n",
      "Epoch 191/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0909 - val_loss: 0.4035\n",
      "Epoch 192/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0918 - val_loss: 0.4029\n",
      "Epoch 193/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0920 - val_loss: 0.4029\n",
      "Epoch 194/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0922 - val_loss: 0.4040\n",
      "Epoch 195/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0903 - val_loss: 0.4043\n",
      "Epoch 196/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0906 - val_loss: 0.4038\n",
      "Epoch 197/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0908 - val_loss: 0.4034\n",
      "Epoch 198/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0904 - val_loss: 0.4043\n",
      "Epoch 199/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0899 - val_loss: 0.4031\n",
      "Epoch 200/200\n",
      "1095/1095 [==============================] - 26s 24ms/step - loss: 0.0903 - val_loss: 0.4048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14640ada0>"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "    [trainAttrX, trainImagesX], trainY,\n",
    "    validation_data=([testAttrX, testImagesX], testY),\n",
    "    epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = df.iloc[testAttrX.index].Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([testAttrX, testImagesX])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NX        2446443.0\n",
       "NL_EX        5134.0\n",
       "NL_IM          -0.0\n",
       "CHE_EX       5521.0\n",
       "CHE_IM         -0.0\n",
       "DNK_EX       2374.0\n",
       "DNK_IM         -0.0\n",
       "CZE_EX       2957.0\n",
       "CZE_IM         -0.0\n",
       "LUX_EX        855.0\n",
       "LUX_IM          0.0\n",
       "SWE_EX        596.0\n",
       "SWE_IM         -0.0\n",
       "AUT_EX    2446691.0\n",
       "AUT_IM         -0.0\n",
       "FRA_EX       3482.0\n",
       "FRA_IM         -0.0\n",
       "PL_EX        2231.0\n",
       "PL_IM          -0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NX</th>\n",
       "      <th>NL_EX</th>\n",
       "      <th>NL_IM</th>\n",
       "      <th>CHE_EX</th>\n",
       "      <th>CHE_IM</th>\n",
       "      <th>DNK_EX</th>\n",
       "      <th>DNK_IM</th>\n",
       "      <th>CZE_EX</th>\n",
       "      <th>CZE_IM</th>\n",
       "      <th>LUX_EX</th>\n",
       "      <th>LUX_IM</th>\n",
       "      <th>SWE_EX</th>\n",
       "      <th>SWE_IM</th>\n",
       "      <th>AUT_EX</th>\n",
       "      <th>AUT_IM</th>\n",
       "      <th>FRA_EX</th>\n",
       "      <th>FRA_IM</th>\n",
       "      <th>PL_EX</th>\n",
       "      <th>PL_IM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10448.128906</td>\n",
       "      <td>1708.713501</td>\n",
       "      <td>1.597458</td>\n",
       "      <td>2848.725098</td>\n",
       "      <td>-17.431534</td>\n",
       "      <td>1029.946777</td>\n",
       "      <td>-140.159821</td>\n",
       "      <td>1179.883179</td>\n",
       "      <td>-388.092773</td>\n",
       "      <td>471.456909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.620113</td>\n",
       "      <td>-19.816391</td>\n",
       "      <td>2495.445801</td>\n",
       "      <td>-4.061020</td>\n",
       "      <td>226.710022</td>\n",
       "      <td>-146.181946</td>\n",
       "      <td>1121.020142</td>\n",
       "      <td>-5.531911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10515.189453</td>\n",
       "      <td>1750.652466</td>\n",
       "      <td>-3.236111</td>\n",
       "      <td>3061.526855</td>\n",
       "      <td>-21.313477</td>\n",
       "      <td>911.272583</td>\n",
       "      <td>-230.084091</td>\n",
       "      <td>1142.236694</td>\n",
       "      <td>-377.538666</td>\n",
       "      <td>448.379852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.415150</td>\n",
       "      <td>-43.588928</td>\n",
       "      <td>2783.275146</td>\n",
       "      <td>-5.226109</td>\n",
       "      <td>297.911621</td>\n",
       "      <td>-329.336670</td>\n",
       "      <td>1105.484375</td>\n",
       "      <td>-3.078765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9744.891602</td>\n",
       "      <td>1481.171997</td>\n",
       "      <td>-11.681484</td>\n",
       "      <td>3141.760254</td>\n",
       "      <td>-14.357994</td>\n",
       "      <td>880.213318</td>\n",
       "      <td>-298.383301</td>\n",
       "      <td>1053.320801</td>\n",
       "      <td>-410.177979</td>\n",
       "      <td>424.004822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.710430</td>\n",
       "      <td>-67.794838</td>\n",
       "      <td>2456.312012</td>\n",
       "      <td>-3.264038</td>\n",
       "      <td>270.672852</td>\n",
       "      <td>-525.889038</td>\n",
       "      <td>1012.708435</td>\n",
       "      <td>-3.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9598.474609</td>\n",
       "      <td>1659.736572</td>\n",
       "      <td>-29.209076</td>\n",
       "      <td>3273.765137</td>\n",
       "      <td>-8.237900</td>\n",
       "      <td>960.281372</td>\n",
       "      <td>-342.721985</td>\n",
       "      <td>987.320129</td>\n",
       "      <td>-516.634033</td>\n",
       "      <td>412.689819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.938942</td>\n",
       "      <td>-92.951454</td>\n",
       "      <td>2549.632324</td>\n",
       "      <td>-9.832916</td>\n",
       "      <td>184.649551</td>\n",
       "      <td>-678.123535</td>\n",
       "      <td>912.247253</td>\n",
       "      <td>-0.709822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9158.412109</td>\n",
       "      <td>1599.301025</td>\n",
       "      <td>-45.179398</td>\n",
       "      <td>3279.538086</td>\n",
       "      <td>-19.619133</td>\n",
       "      <td>981.457642</td>\n",
       "      <td>-368.226837</td>\n",
       "      <td>1045.816284</td>\n",
       "      <td>-491.043457</td>\n",
       "      <td>418.855377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.267790</td>\n",
       "      <td>-96.548050</td>\n",
       "      <td>2740.553223</td>\n",
       "      <td>-8.567661</td>\n",
       "      <td>127.996155</td>\n",
       "      <td>-652.598389</td>\n",
       "      <td>929.010864</td>\n",
       "      <td>-1.640959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9951.730469</td>\n",
       "      <td>1557.167847</td>\n",
       "      <td>-48.155853</td>\n",
       "      <td>3457.994629</td>\n",
       "      <td>-9.869385</td>\n",
       "      <td>1042.714478</td>\n",
       "      <td>-335.137146</td>\n",
       "      <td>1149.955200</td>\n",
       "      <td>-504.719055</td>\n",
       "      <td>443.553894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.442503</td>\n",
       "      <td>-92.439629</td>\n",
       "      <td>2583.129883</td>\n",
       "      <td>-8.982166</td>\n",
       "      <td>113.623596</td>\n",
       "      <td>-513.467163</td>\n",
       "      <td>980.940979</td>\n",
       "      <td>-1.307303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9295.516602</td>\n",
       "      <td>1340.922607</td>\n",
       "      <td>-71.620598</td>\n",
       "      <td>3032.127930</td>\n",
       "      <td>-10.235703</td>\n",
       "      <td>1170.584961</td>\n",
       "      <td>-301.292236</td>\n",
       "      <td>1284.227905</td>\n",
       "      <td>-404.730225</td>\n",
       "      <td>453.562927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.986160</td>\n",
       "      <td>-102.707970</td>\n",
       "      <td>2194.591064</td>\n",
       "      <td>-0.290833</td>\n",
       "      <td>154.591400</td>\n",
       "      <td>-498.838928</td>\n",
       "      <td>951.059387</td>\n",
       "      <td>0.985971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7882.916016</td>\n",
       "      <td>1640.194580</td>\n",
       "      <td>-49.154251</td>\n",
       "      <td>2311.482910</td>\n",
       "      <td>-33.119370</td>\n",
       "      <td>990.539612</td>\n",
       "      <td>-311.287415</td>\n",
       "      <td>1270.569092</td>\n",
       "      <td>-461.409546</td>\n",
       "      <td>511.714355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.193052</td>\n",
       "      <td>-198.288147</td>\n",
       "      <td>2114.054688</td>\n",
       "      <td>-42.465363</td>\n",
       "      <td>216.093979</td>\n",
       "      <td>-274.574219</td>\n",
       "      <td>891.252441</td>\n",
       "      <td>4.104674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8042.710938</td>\n",
       "      <td>1666.795654</td>\n",
       "      <td>-11.039646</td>\n",
       "      <td>2081.747314</td>\n",
       "      <td>-48.308235</td>\n",
       "      <td>1048.950806</td>\n",
       "      <td>-361.991730</td>\n",
       "      <td>1370.337280</td>\n",
       "      <td>-500.603058</td>\n",
       "      <td>525.519836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.765181</td>\n",
       "      <td>-158.228058</td>\n",
       "      <td>1678.393433</td>\n",
       "      <td>-71.052597</td>\n",
       "      <td>266.971283</td>\n",
       "      <td>-305.255310</td>\n",
       "      <td>911.587952</td>\n",
       "      <td>6.681344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8906.343750</td>\n",
       "      <td>1994.093628</td>\n",
       "      <td>-6.303020</td>\n",
       "      <td>2149.482910</td>\n",
       "      <td>1.934235</td>\n",
       "      <td>1012.336304</td>\n",
       "      <td>-327.606079</td>\n",
       "      <td>1469.541260</td>\n",
       "      <td>-480.556091</td>\n",
       "      <td>557.163818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.687788</td>\n",
       "      <td>-140.050262</td>\n",
       "      <td>1787.954956</td>\n",
       "      <td>-45.112503</td>\n",
       "      <td>352.986938</td>\n",
       "      <td>-247.359802</td>\n",
       "      <td>887.966125</td>\n",
       "      <td>7.120664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9429.105469</td>\n",
       "      <td>2021.048096</td>\n",
       "      <td>-1.039215</td>\n",
       "      <td>2121.104492</td>\n",
       "      <td>20.064941</td>\n",
       "      <td>958.214783</td>\n",
       "      <td>-313.919434</td>\n",
       "      <td>1525.455566</td>\n",
       "      <td>-431.548157</td>\n",
       "      <td>580.866760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.666288</td>\n",
       "      <td>-148.533295</td>\n",
       "      <td>2030.538574</td>\n",
       "      <td>-40.273808</td>\n",
       "      <td>316.854034</td>\n",
       "      <td>-288.096680</td>\n",
       "      <td>925.383118</td>\n",
       "      <td>5.355332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9557.478516</td>\n",
       "      <td>1852.878906</td>\n",
       "      <td>0.333942</td>\n",
       "      <td>2230.527588</td>\n",
       "      <td>44.443420</td>\n",
       "      <td>1040.395020</td>\n",
       "      <td>-339.037231</td>\n",
       "      <td>1531.203125</td>\n",
       "      <td>-388.884003</td>\n",
       "      <td>588.365051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.779461</td>\n",
       "      <td>-160.803406</td>\n",
       "      <td>2242.190186</td>\n",
       "      <td>-22.048149</td>\n",
       "      <td>294.766541</td>\n",
       "      <td>-303.458862</td>\n",
       "      <td>865.459351</td>\n",
       "      <td>5.556483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9659.112305</td>\n",
       "      <td>1685.906982</td>\n",
       "      <td>1.078949</td>\n",
       "      <td>2207.919922</td>\n",
       "      <td>30.378571</td>\n",
       "      <td>1116.070190</td>\n",
       "      <td>-371.020996</td>\n",
       "      <td>1577.721680</td>\n",
       "      <td>-331.935364</td>\n",
       "      <td>576.919434</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432880</td>\n",
       "      <td>-141.401550</td>\n",
       "      <td>2245.555664</td>\n",
       "      <td>-19.706528</td>\n",
       "      <td>270.700287</td>\n",
       "      <td>-377.266052</td>\n",
       "      <td>907.217529</td>\n",
       "      <td>3.925960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9622.456055</td>\n",
       "      <td>1661.686035</td>\n",
       "      <td>-0.402798</td>\n",
       "      <td>2406.792480</td>\n",
       "      <td>11.896500</td>\n",
       "      <td>1175.780273</td>\n",
       "      <td>-378.084503</td>\n",
       "      <td>1559.592529</td>\n",
       "      <td>-344.552612</td>\n",
       "      <td>566.284180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.497952</td>\n",
       "      <td>-142.552887</td>\n",
       "      <td>2382.901367</td>\n",
       "      <td>-16.488327</td>\n",
       "      <td>293.344391</td>\n",
       "      <td>-430.553650</td>\n",
       "      <td>857.580688</td>\n",
       "      <td>2.598180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9202.086914</td>\n",
       "      <td>1497.969604</td>\n",
       "      <td>-7.823883</td>\n",
       "      <td>2497.406006</td>\n",
       "      <td>1.118423</td>\n",
       "      <td>1120.728882</td>\n",
       "      <td>-418.006866</td>\n",
       "      <td>1595.463135</td>\n",
       "      <td>-351.909485</td>\n",
       "      <td>548.113220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.083504</td>\n",
       "      <td>-169.207092</td>\n",
       "      <td>2342.333984</td>\n",
       "      <td>-10.643883</td>\n",
       "      <td>228.034073</td>\n",
       "      <td>-483.410278</td>\n",
       "      <td>880.509949</td>\n",
       "      <td>0.427573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8648.967773</td>\n",
       "      <td>1401.349854</td>\n",
       "      <td>-7.795013</td>\n",
       "      <td>2428.646729</td>\n",
       "      <td>-8.946365</td>\n",
       "      <td>1214.019531</td>\n",
       "      <td>-469.266632</td>\n",
       "      <td>1600.750488</td>\n",
       "      <td>-362.436249</td>\n",
       "      <td>532.894165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.900749</td>\n",
       "      <td>-205.497849</td>\n",
       "      <td>2234.283447</td>\n",
       "      <td>-27.491711</td>\n",
       "      <td>175.891541</td>\n",
       "      <td>-657.252319</td>\n",
       "      <td>847.024719</td>\n",
       "      <td>0.672931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8334.188477</td>\n",
       "      <td>1606.046509</td>\n",
       "      <td>-7.827934</td>\n",
       "      <td>2255.080811</td>\n",
       "      <td>-12.962784</td>\n",
       "      <td>1172.345703</td>\n",
       "      <td>-536.059143</td>\n",
       "      <td>1541.779297</td>\n",
       "      <td>-381.776428</td>\n",
       "      <td>522.642639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.137070</td>\n",
       "      <td>-161.980728</td>\n",
       "      <td>1815.773804</td>\n",
       "      <td>-39.203835</td>\n",
       "      <td>147.476669</td>\n",
       "      <td>-742.142517</td>\n",
       "      <td>907.016785</td>\n",
       "      <td>2.017730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7444.658203</td>\n",
       "      <td>1740.417358</td>\n",
       "      <td>-0.729885</td>\n",
       "      <td>1791.399048</td>\n",
       "      <td>-39.076332</td>\n",
       "      <td>1061.804932</td>\n",
       "      <td>-591.642334</td>\n",
       "      <td>1501.028931</td>\n",
       "      <td>-423.528961</td>\n",
       "      <td>539.222168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.165274</td>\n",
       "      <td>-132.621277</td>\n",
       "      <td>1745.015137</td>\n",
       "      <td>-121.731453</td>\n",
       "      <td>112.549515</td>\n",
       "      <td>-586.090332</td>\n",
       "      <td>901.234253</td>\n",
       "      <td>1.232560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6686.824219</td>\n",
       "      <td>1786.571411</td>\n",
       "      <td>-5.172256</td>\n",
       "      <td>1648.900269</td>\n",
       "      <td>-72.205856</td>\n",
       "      <td>942.186707</td>\n",
       "      <td>-628.609131</td>\n",
       "      <td>1398.564453</td>\n",
       "      <td>-493.498901</td>\n",
       "      <td>548.473328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.962280</td>\n",
       "      <td>-83.442101</td>\n",
       "      <td>1490.741699</td>\n",
       "      <td>-134.318359</td>\n",
       "      <td>194.572372</td>\n",
       "      <td>-331.466614</td>\n",
       "      <td>841.926758</td>\n",
       "      <td>0.271871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7153.302734</td>\n",
       "      <td>1835.331909</td>\n",
       "      <td>-3.679207</td>\n",
       "      <td>1741.217041</td>\n",
       "      <td>-107.854576</td>\n",
       "      <td>896.323853</td>\n",
       "      <td>-627.291626</td>\n",
       "      <td>1416.386719</td>\n",
       "      <td>-526.658997</td>\n",
       "      <td>526.080017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.549994</td>\n",
       "      <td>-143.372864</td>\n",
       "      <td>1179.455811</td>\n",
       "      <td>-33.353302</td>\n",
       "      <td>299.991150</td>\n",
       "      <td>-460.071411</td>\n",
       "      <td>868.147217</td>\n",
       "      <td>-0.748055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6926.081055</td>\n",
       "      <td>1679.099976</td>\n",
       "      <td>-6.161129</td>\n",
       "      <td>2158.988281</td>\n",
       "      <td>-90.287384</td>\n",
       "      <td>872.737000</td>\n",
       "      <td>-756.168945</td>\n",
       "      <td>1422.019531</td>\n",
       "      <td>-489.417053</td>\n",
       "      <td>499.620483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.339102</td>\n",
       "      <td>-229.209671</td>\n",
       "      <td>1976.256226</td>\n",
       "      <td>4.369492</td>\n",
       "      <td>158.396240</td>\n",
       "      <td>-634.835815</td>\n",
       "      <td>959.726685</td>\n",
       "      <td>0.801273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7349.786621</td>\n",
       "      <td>1467.522461</td>\n",
       "      <td>0.431458</td>\n",
       "      <td>2495.203857</td>\n",
       "      <td>-74.509262</td>\n",
       "      <td>912.470215</td>\n",
       "      <td>-741.300659</td>\n",
       "      <td>1259.783691</td>\n",
       "      <td>-500.177765</td>\n",
       "      <td>480.080963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.645786</td>\n",
       "      <td>-193.193253</td>\n",
       "      <td>2012.090576</td>\n",
       "      <td>-24.434469</td>\n",
       "      <td>84.556229</td>\n",
       "      <td>-627.831299</td>\n",
       "      <td>925.894531</td>\n",
       "      <td>-0.710429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7771.655762</td>\n",
       "      <td>1530.473877</td>\n",
       "      <td>-0.790878</td>\n",
       "      <td>2577.569824</td>\n",
       "      <td>-63.175964</td>\n",
       "      <td>908.172241</td>\n",
       "      <td>-697.997803</td>\n",
       "      <td>1157.674194</td>\n",
       "      <td>-533.788452</td>\n",
       "      <td>504.269287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.037605</td>\n",
       "      <td>-180.450043</td>\n",
       "      <td>2197.007324</td>\n",
       "      <td>-26.490625</td>\n",
       "      <td>148.024338</td>\n",
       "      <td>-455.262207</td>\n",
       "      <td>998.402832</td>\n",
       "      <td>1.589751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8736.677734</td>\n",
       "      <td>1840.677856</td>\n",
       "      <td>-0.669697</td>\n",
       "      <td>2756.704102</td>\n",
       "      <td>-6.583313</td>\n",
       "      <td>1028.080444</td>\n",
       "      <td>-529.749756</td>\n",
       "      <td>999.843506</td>\n",
       "      <td>-632.832153</td>\n",
       "      <td>475.023071</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.339340</td>\n",
       "      <td>-163.986481</td>\n",
       "      <td>2175.927979</td>\n",
       "      <td>-12.948723</td>\n",
       "      <td>271.679932</td>\n",
       "      <td>-358.994141</td>\n",
       "      <td>912.170288</td>\n",
       "      <td>2.368360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              NX        NL_EX      NL_IM       CHE_EX      CHE_IM  \\\n",
       "0   10448.128906  1708.713501   1.597458  2848.725098  -17.431534   \n",
       "1   10515.189453  1750.652466  -3.236111  3061.526855  -21.313477   \n",
       "2    9744.891602  1481.171997 -11.681484  3141.760254  -14.357994   \n",
       "3    9598.474609  1659.736572 -29.209076  3273.765137   -8.237900   \n",
       "4    9158.412109  1599.301025 -45.179398  3279.538086  -19.619133   \n",
       "5    9951.730469  1557.167847 -48.155853  3457.994629   -9.869385   \n",
       "6    9295.516602  1340.922607 -71.620598  3032.127930  -10.235703   \n",
       "7    7882.916016  1640.194580 -49.154251  2311.482910  -33.119370   \n",
       "8    8042.710938  1666.795654 -11.039646  2081.747314  -48.308235   \n",
       "9    8906.343750  1994.093628  -6.303020  2149.482910    1.934235   \n",
       "10   9429.105469  2021.048096  -1.039215  2121.104492   20.064941   \n",
       "11   9557.478516  1852.878906   0.333942  2230.527588   44.443420   \n",
       "12   9659.112305  1685.906982   1.078949  2207.919922   30.378571   \n",
       "13   9622.456055  1661.686035  -0.402798  2406.792480   11.896500   \n",
       "14   9202.086914  1497.969604  -7.823883  2497.406006    1.118423   \n",
       "15   8648.967773  1401.349854  -7.795013  2428.646729   -8.946365   \n",
       "16   8334.188477  1606.046509  -7.827934  2255.080811  -12.962784   \n",
       "17   7444.658203  1740.417358  -0.729885  1791.399048  -39.076332   \n",
       "18   6686.824219  1786.571411  -5.172256  1648.900269  -72.205856   \n",
       "19   7153.302734  1835.331909  -3.679207  1741.217041 -107.854576   \n",
       "20   6926.081055  1679.099976  -6.161129  2158.988281  -90.287384   \n",
       "21   7349.786621  1467.522461   0.431458  2495.203857  -74.509262   \n",
       "22   7771.655762  1530.473877  -0.790878  2577.569824  -63.175964   \n",
       "23   8736.677734  1840.677856  -0.669697  2756.704102   -6.583313   \n",
       "\n",
       "         DNK_EX      DNK_IM       CZE_EX      CZE_IM      LUX_EX  LUX_IM  \\\n",
       "0   1029.946777 -140.159821  1179.883179 -388.092773  471.456909     0.0   \n",
       "1    911.272583 -230.084091  1142.236694 -377.538666  448.379852     0.0   \n",
       "2    880.213318 -298.383301  1053.320801 -410.177979  424.004822     0.0   \n",
       "3    960.281372 -342.721985   987.320129 -516.634033  412.689819     0.0   \n",
       "4    981.457642 -368.226837  1045.816284 -491.043457  418.855377     0.0   \n",
       "5   1042.714478 -335.137146  1149.955200 -504.719055  443.553894     0.0   \n",
       "6   1170.584961 -301.292236  1284.227905 -404.730225  453.562927     0.0   \n",
       "7    990.539612 -311.287415  1270.569092 -461.409546  511.714355     0.0   \n",
       "8   1048.950806 -361.991730  1370.337280 -500.603058  525.519836     0.0   \n",
       "9   1012.336304 -327.606079  1469.541260 -480.556091  557.163818     0.0   \n",
       "10   958.214783 -313.919434  1525.455566 -431.548157  580.866760     0.0   \n",
       "11  1040.395020 -339.037231  1531.203125 -388.884003  588.365051     0.0   \n",
       "12  1116.070190 -371.020996  1577.721680 -331.935364  576.919434     0.0   \n",
       "13  1175.780273 -378.084503  1559.592529 -344.552612  566.284180     0.0   \n",
       "14  1120.728882 -418.006866  1595.463135 -351.909485  548.113220     0.0   \n",
       "15  1214.019531 -469.266632  1600.750488 -362.436249  532.894165     0.0   \n",
       "16  1172.345703 -536.059143  1541.779297 -381.776428  522.642639     0.0   \n",
       "17  1061.804932 -591.642334  1501.028931 -423.528961  539.222168     0.0   \n",
       "18   942.186707 -628.609131  1398.564453 -493.498901  548.473328     0.0   \n",
       "19   896.323853 -627.291626  1416.386719 -526.658997  526.080017     0.0   \n",
       "20   872.737000 -756.168945  1422.019531 -489.417053  499.620483     0.0   \n",
       "21   912.470215 -741.300659  1259.783691 -500.177765  480.080963     0.0   \n",
       "22   908.172241 -697.997803  1157.674194 -533.788452  504.269287     0.0   \n",
       "23  1028.080444 -529.749756   999.843506 -632.832153  475.023071     0.0   \n",
       "\n",
       "       SWE_EX      SWE_IM       AUT_EX      AUT_IM      FRA_EX      FRA_IM  \\\n",
       "0   44.620113  -19.816391  2495.445801   -4.061020  226.710022 -146.181946   \n",
       "1   49.415150  -43.588928  2783.275146   -5.226109  297.911621 -329.336670   \n",
       "2   35.710430  -67.794838  2456.312012   -3.264038  270.672852 -525.889038   \n",
       "3   20.938942  -92.951454  2549.632324   -9.832916  184.649551 -678.123535   \n",
       "4   29.267790  -96.548050  2740.553223   -8.567661  127.996155 -652.598389   \n",
       "5   31.442503  -92.439629  2583.129883   -8.982166  113.623596 -513.467163   \n",
       "6   16.986160 -102.707970  2194.591064   -0.290833  154.591400 -498.838928   \n",
       "7   27.193052 -198.288147  2114.054688  -42.465363  216.093979 -274.574219   \n",
       "8   21.765181 -158.228058  1678.393433  -71.052597  266.971283 -305.255310   \n",
       "9   27.687788 -140.050262  1787.954956  -45.112503  352.986938 -247.359802   \n",
       "10  26.666288 -148.533295  2030.538574  -40.273808  316.854034 -288.096680   \n",
       "11  13.779461 -160.803406  2242.190186  -22.048149  294.766541 -303.458862   \n",
       "12   0.432880 -141.401550  2245.555664  -19.706528  270.700287 -377.266052   \n",
       "13  -6.497952 -142.552887  2382.901367  -16.488327  293.344391 -430.553650   \n",
       "14   5.083504 -169.207092  2342.333984  -10.643883  228.034073 -483.410278   \n",
       "15  14.900749 -205.497849  2234.283447  -27.491711  175.891541 -657.252319   \n",
       "16  40.137070 -161.980728  1815.773804  -39.203835  147.476669 -742.142517   \n",
       "17  27.165274 -132.621277  1745.015137 -121.731453  112.549515 -586.090332   \n",
       "18  10.962280  -83.442101  1490.741699 -134.318359  194.572372 -331.466614   \n",
       "19  22.549994 -143.372864  1179.455811  -33.353302  299.991150 -460.071411   \n",
       "20  26.339102 -229.209671  1976.256226    4.369492  158.396240 -634.835815   \n",
       "21  22.645786 -193.193253  2012.090576  -24.434469   84.556229 -627.831299   \n",
       "22  25.037605 -180.450043  2197.007324  -26.490625  148.024338 -455.262207   \n",
       "23  42.339340 -163.986481  2175.927979  -12.948723  271.679932 -358.994141   \n",
       "\n",
       "          PL_EX     PL_IM  \n",
       "0   1121.020142 -5.531911  \n",
       "1   1105.484375 -3.078765  \n",
       "2   1012.708435 -3.666100  \n",
       "3    912.247253 -0.709822  \n",
       "4    929.010864 -1.640959  \n",
       "5    980.940979 -1.307303  \n",
       "6    951.059387  0.985971  \n",
       "7    891.252441  4.104674  \n",
       "8    911.587952  6.681344  \n",
       "9    887.966125  7.120664  \n",
       "10   925.383118  5.355332  \n",
       "11   865.459351  5.556483  \n",
       "12   907.217529  3.925960  \n",
       "13   857.580688  2.598180  \n",
       "14   880.509949  0.427573  \n",
       "15   847.024719  0.672931  \n",
       "16   907.016785  2.017730  \n",
       "17   901.234253  1.232560  \n",
       "18   841.926758  0.271871  \n",
       "19   868.147217 -0.748055  \n",
       "20   959.726685  0.801273  \n",
       "21   925.894531 -0.710429  \n",
       "22   998.402832  1.589751  \n",
       "23   912.170288  2.368360  "
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.DataFrame(preds[0],columns=df.iloc[:,2::].columns)*std_columns)  + mean_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
